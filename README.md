<div align="center">
    <h1>Formula Image Latex Recognition</h1>
    <img src="assets/logo.png" alt="logo"/>
    <br/>
    <img src="https://img.shields.io/github/stars/pstage-ocr-team6/ocr-teamcode?color=yellow" alt="Star"/>
    <img src="https://img.shields.io/github/forks/pstage-ocr-team6/ocr-teamcode?color=green" alt="Forks">
    <img src="https://img.shields.io/github/issues/pstage-ocr-team6/ocr-teamcode?color=red" alt="Issues"/>
    <img src="https://img.shields.io/github/license/pstage-ocr-team6/ocr-teamcode" alt="License"/>
</div>

---

## ğŸ“ Table of Contents

- [Latex Recognition Task](#-latex-recognition-task)
- [Installation](#-installation)
- [File Structure](#-file-structure)
- [Getting Started](#-getting-started)
  - [Installation](#installation)
  - [Dataset Setting](#dataset-setting)
  - [Config Setting](#config-setting)
- [Usage](#-usage)
  - [Train](#train)
  - [Inference](#inference)
- [Demo](#-demo)
- [References](#-references)
- [Contributors](#-contributors)
- [License](#-license)

---

## â— Latex Recognition Task

<div align="center">
  <img src="assets/competition-overview.png" alt="Competition Overview"/>
</div>

ìˆ˜ì‹ ì¸ì‹(Latex Recognition)ì€ **ìˆ˜ì‹ ì´ë¯¸ì§€ì—ì„œ LaTeX í¬ë§·ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ëŠ” íƒœìŠ¤í¬**ë¡œ, ë¬¸ì ì¸ì‹(Character Recognition)ê³¼ ë‹¬ë¦¬ ìˆ˜ì‹ ì¸ì‹ì˜ ê²½ìš° `ì¢Œ â†’ ìš°` ë¿ë§Œ ì•„ë‹ˆë¼ Multi-lineì— ëŒ€í•´ì„œ `ìœ„ â†’ ì•„ë˜`ì— ëŒ€í•œ ìˆœì„œ íŒ¨í„´ í•™ìŠµë„ í•„ìš”í•˜ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.

## ğŸ“ File Structure

### Code Folder

```text
ocr_teamcode/
â”‚
â”œâ”€â”€ config/                   # train argument config file
â”‚   â”œâ”€â”€ Attention.yaml
â”‚   â””â”€â”€ SATRN.yaml
â”‚
â”œâ”€â”€ data_tools/               # utils for dataset
â”‚   â”œâ”€â”€ download.sh           # dataset download script
â”‚   â”œâ”€â”€ extract_tokens.py     # extract tokens from token.txt
â”‚   â”œâ”€â”€ make_dataset.py       # sample dataset
â”‚   â”œâ”€â”€ parse_upstage.py      # convert JSON ground truth file to ICDAR15 format
â”‚   â””â”€â”€ train_test_split.py   # split dataset into train and test dataset
â”‚
â”œâ”€â”€ networks/                 # network, loss
â”‚   â”œâ”€â”€ Attention.py
â”‚   â”œâ”€â”€ SATRN.py
â”‚   â””â”€â”€ loss.py
â”‚   â””â”€â”€ spatial_transformation.py
â”‚
â”œâ”€â”€ checkpoint.py             # save, load checkpoints
â”œâ”€â”€ pre_processing.py         # preprocess images with OpenCV
â”œâ”€â”€ custom_augment.py         # image augmentations
â”œâ”€â”€ transform.py
â”œâ”€â”€ dataset.py
â”œâ”€â”€ flags.py                  # parse yaml to FLAG format
â”œâ”€â”€ inference.py              # inference
â”œâ”€â”€ metrics.py                # calculate evaluation metrics
â”œâ”€â”€ scheduler.py              # learning rate scheduler
â”œâ”€â”€ train.py                  # train
â””â”€â”€ utils.py                  # utils for training
```

### Dataset Folder

```text
input/data/train_dataset
â”‚
â”œâ”€â”€ images/                 # input image folder
â”‚   â”œâ”€â”€ train_00000.jpg
â”‚   â”œâ”€â”€ train_00001.jpg
â”‚   â”œâ”€â”€ train_00002.jpg
â”‚   â””â”€â”€ ...
|
â”œâ”€â”€ gt.txt                  # input data
â”œâ”€â”€ level.txt               # formula difficulty feature
â”œâ”€â”€ source.txt              # printed output / hand written feature
â””â”€â”€ tokens.txt              # vocabulary for training
```

## âœ¨ Getting Started

### Installation

```shell
pip install -r requirements.txt
```

- scikit_image==0.14.1
- opencv_python==3.4.4.19
- tqdm==4.28.1
- torch==1.7.1+cu101
- torchvision==0.8.2+cu101
- scipy==1.2.0
- numpy==1.15.4
- pillow==8.2.0
- tensorboardX==1.5
- editdistance==0.5.3
- python-dotenv==0.17.1
- wandb==0.10.30
- adamp==0.3.0
- python-dotenv==0.17.1

### Download Dataset

```shell
sh filename.sh
```

### Dataset Setting

> <b>ğŸ“Œ í•™ìŠµë°ì´í„°ëŠ” [Dataset Folder](#dataset-folder)ì™€ ê°™ì´ ë„£ì–´ì£¼ì„¸ìš”!</b>

> <b>ğŸ“Œ ë‹¨ì¼ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ëœ txtëŠ” `\n`ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ë©°, 2ê°œ ì´ìƒì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ëœ txtëŠ” `\t`ë¡œ ì»¬ëŸ¼ì„, `\n`ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.</b>

í•™ìŠµë°ì´í„°ëŠ” `tokens.txt`, `gt.txt`, `level.txt`, `source.txt` ì´ 4ê°œì˜ íŒŒì¼ê³¼ ì´ë¯¸ì§€ í´ë”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì´ ì¤‘ `tokens.txt`ì™€ `gt.txt`ëŠ” **ëª¨ë¸ í•™ìŠµ**ì— ê¼­ í•„ìš”í•œ ì…ë ¥ íŒŒì¼ì´ë©°, `level.txt`, `source.txt`ëŠ” ì´ë¯¸ì§€ì— ëŒ€í•œ ë©”íƒ€ ë°ì´í„°ë¡œ **ë°ì´í„°ì…‹ ë¶„ë¦¬**ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- `tokens.txt`ëŠ” **í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” vocabulary íŒŒì¼**ë¡œì„œ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ tokenë“¤ì´ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

  ```text
  O
  \prod
  \downarrow
  ...
  ```

- `gt.txt`ëŠ” **ì‹¤ì œ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” íŒŒì¼**ë¡œ ì´ë¯¸ì§€ ê²½ë¡œ, LaTexë¡œ ëœ Ground Truthë¡œ ê° ì»¬ëŸ¼ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

  ```text
  train_00000.jpg	4 \times 7 = 2 8
  train_00001.jpg	a ^ { x } > q
  train_00002.jpg	8 \times 9
  ...
  ```

- `level.txt`ëŠ” **ìˆ˜ì‹ì˜ ë‚œì´ë„ ì •ë³´ íŒŒì¼**ë¡œ ê° ì»¬ëŸ¼ì€ ê²½ë¡œì™€ ë‚œì´ë„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ìˆ«ìëŠ” 1(ì´ˆë“±), 2(ì¤‘ë“±), 3(ê³ ë“±), 4(ëŒ€í•™), 5(ëŒ€í•™ ì´ìƒ)ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

  ```text
  train_00000.jpg	1
  train_00001.jpg	2
  train_00002.jpg	2
  ...
  ```

- `source.txt`ëŠ” ì´ë¯¸ì§€ì˜ ì¶œë ¥ í˜•íƒœ ì •ë³´ íŒŒì¼ë¡œ, ì»¬ëŸ¼ì€ ê²½ë¡œì™€ ì†ŒìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ìˆ«ìëŠ” 0(í”„ë¦°íŠ¸ ì¶œë ¥ë¬¼), 1(ì†ê¸€ì”¨)ë¥¼ ëœ»í•©ë‹ˆë‹¤.

  ```text
  train_00000.jpg	1
  train_00001.jpg	0
  train_00002.jpg	0
  ```

### Create .env for wandb

wandb loggingì„ ì‚¬ìš© ì‹œ wandbì— ë„˜ê²¨ì£¼ì–´ì•¼ í•˜ëŠ” ì¸ìë¥¼ `.env` íŒŒì¼ì— ì •ì˜í•©ë‹ˆë‹¤.

```
PROJECT="[wandb project name]"
ENTITY="[wandb nickname]"
```

### Config Setting

í•™ìŠµ ì‹œ ì‚¬ìš©í•˜ëŠ” config íŒŒì¼ì€ `yaml`íŒŒì¼ë¡œ í•™ìŠµ ëª©í‘œì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•´ì£¼ì„¸ìš”.

```yaml
network: SATRN
input_size: # resize image
  height: 48
  width: 192
SATRN:
  encoder:
    hidden_dim: 300
    filter_dim: 1200
    layer_num: 6
    head_num: 8

    shallower_cnn: True # shallow CNN
    adaptive_gate: True # A2DPE
    conv_ff: True # locality-aware feedforward
    separable_ff: True # only if conv_ff is True
  decoder:
    src_dim: 300
    hidden_dim: 300
    filter_dim: 1200
    layer_num: 3
    head_num: 8

checkpoint: "" # load checkpoint
prefix: "./log/satrn" # log folder name

data:
  train: # train dataset file path
    - "/opt/ml/input/data/train_dataset/gt.txt"
  test: # validation dataset file path
    -
  token_paths: # token file path
    - "/opt/ml/input/data/train_dataset/tokens.txt" # 241 tokens
  dataset_proportions: # proportion of data to take from train (not test)
    - 1.0
  random_split: True # if True, random split from train files
  test_proportions: 0.2 # only if random_split is True, create validation set
  crop: True # center crop image
  rgb: 1 # 3 for color, 1 for greyscale

batch_size: 16
num_workers: 8
num_epochs: 200
print_epochs: 1 # print interval
dropout_rate: 0.1
teacher_forcing_ratio: 0.5 # teacher forcing ratio
teacher_forcing_damp: 5e-3 # teacher forcing decay (0 to turn off)
max_grad_norm: 2.0 # gradient clipping
seed: 1234
optimizer:
  optimizer: AdamP
  lr: 5e-4
  weight_decay: 1e-4
  selective_weight_decay: True # no decay in norm and bias
  is_cycle: True # cyclic learning rate scheduler
label_smoothing: 0.2 # label smoothing factor (0 to off)

patience: 30 # stop train after waiting (-1 for off)
save_best_only: True # save best model only

fp16: True # mixed precision

wandb:
  wandb: True # wandb logging
  run_name: "sample_run" # wandb project run name
```

## â© Usage

### Train

```shell
python train.py [--config_file]
```

- `--config_file`: config íŒŒì¼ ê²½ë¡œ

### Inference

```shell
python inference.py [--checkpoint] [--max_sequence] [--batch_size] [--file_path] [--output_dir]
```

- `--checkpoint`: checkpoint íŒŒì¼ ê²½ë¡œ
- `--max_sequence`: inference ì‹œ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
- `--batch_size`: ë°°ì¹˜ í¬ê¸°
- `--file_path`: test dataset ê²½ë¡œ
- `--output_dir`: inference ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

## ğŸš€ Demo

<div align="center">
<img src="assets/demo.png" alt="demo" width="80%">
</div>

## ğŸ“– References

- <i>On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention, Lee et al., 2019</i>
- <i>Bag of Tricks for Image Classiô™Ÿcation with Convolutional Neural Networks, He et al., 2018</i>
- <i>Averaging Weights Leads to Wider Optima and Better Generalization, Izmailov et al., 2018</i>
- <i>CSTR: Revisiting Classification Perspective on Scene Text Recognition, Cai et al., 2021</i>
- <i>Improvement of End-to-End Offline Handwritten Mathematical Expression Recognition by Weakly Supervised
  Learning, Truong et al., 2020</i>
- <i>ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators, Clark et al., 2020</i>
- <i>SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition, Qiao et al., 2020</i>
- <i>Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition,
  Fang et al., 2021</i>
- <i>Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Wu et
  al., 2016</i>

## ğŸ‘©â€ğŸ’» Contributors

|                           **[ê¹€ì¢…ì˜](https://github.com/kjy93217)**                            |                           **[ë¯¼ì§€ì›](https://github.com/peacecheejecake)**                            |                                                    **[ë°•ì†Œí˜„](https://github.com/CoodingPenguin)**                                                    |                              **[ë°°ìˆ˜ë¯¼](https://github.com/bsm8734)**                               |                           **[ì˜¤ì„¸ë¯¼](https://github.com/osmosm7)**                            |                              **[ìµœì¬í˜](https://github.com/opijae)**                               |
| :--------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------: |
| [![Avatar](https://avatars.githubusercontent.com/u/39907037?v=4)](https://github.com/kjy93217) | [![Avatar](https://avatars.githubusercontent.com/u/29668380?v=4)](https://github.com/peacecheejecake) | [![Avatar](https://avatars.githubusercontent.com/u/37505775?s=460&u=44732fef53503e63d47192ce5c2de747eff5f0c6&v=4)](https://github.com/CoodingPenguin) | [![Avatar](https://avatars.githubusercontent.com/u/35002768?s=460&v=4)](https://github.com/bsm8734) | [![Avatar](https://avatars.githubusercontent.com/u/48181287?v=4)](https://github.com/osmosm7) | [![Avatar](https://avatars.githubusercontent.com/u/26226101?s=460&v=4)](https://github.com/opijae) |

## âœ… License

Distributed under the MIT License. See `LICENSE` for more information.
